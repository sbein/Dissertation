\FloatBarrier
\chapter{Summary and Conclusion}
Our understanding of the nature of electroweak symmetry breaking may be nearing a turning point. If no evidence of new phenomena at energy scales at or around 1 TeV comes to light at the LHC, there appears to be fine tuning in the Standard Model. This would be an extraordinary finding, since fine tuning is typically thought to arise only in systems of which multiple similar copies of the system exist ``somewhere''. Considering that the system in question is the universe, then the most acceptable explanation would seem to be that there exist other copies of the universe ``somewhere''. Supersymmetry is a proposed symmetry, which if realized in the context of the Standard Model, could explain the apparent fine tuning in the Standard Model as a natural, and unavoidable, consequence of the laws of physics. The hypothesis of supersymmetry that is consistent with our current understanding of the universe predicts the existence of at least 30 new fundamental fields, many of which would manifest particles that could potentially be detected in proton collisions at the LHC. 

One of the most daunting challenges in making definitive statements about supersymmetry (SUSY) is that there are a multitude of ways in which SUSY could manifest itself. The values of the masses of the new particles could be ordered in any one of an inconceivably large number of permutations. This means that the evidence for SUSY could come a large variety of ways, and it is difficult to be sure if, in the absence of evidence, there is evidence of absence. Searches for supersymmetry at the LHC are typically guided by simplified models, which hypothesize that only two or three of the new particles are present in the collisions of the LHC, and the rest are ignored, or assumed to take on very large masses. So far, after a large number of searches performed by the CMS and ATLAS experiments and interpreted in terms of the simplified models, there is absolutely no hint of supersymmetry. Many so-called natural supersymmetry models have been ruled out. However, given the limited scope of the models, it can be difficult to argue for or against the weak-scale SUSY hypothesis.

To begin to address fundamental limitations of interpreting results in the context of simplified models, I contributed to an analysis that interpreted the results of 7 and 8 TeV CMS searches in terms of a SUSY model that more compressively covers the set of potential particle spectra allowed by the MSSM, called the phenomenological minimal supersymmetric Standard Model (pMSSM)~\cite{Khachatryan:2016nvf}. A number of reasonable assumptions about the MSSM are made which, along with the incorporation of results from past experiments and non-CMS data, constrain the MSSM to a model with just 19 free parameters. The pMSSM is thus an excellent proxy for the MSSM that captures most of the MSSM's phenomenological features that are relevant at the LHC. 

Several important lessons were learned about the viability of the supersymmetric hypothesis given the constraints of CMS 7 and 8 TeV analyses. For example, it was found that gluinos, which, in the simplified model paradigm are found to be excluded below a mass of about 1.2 TeV, are viable in the pMSSM with masses as low as 600 GeV. Scenarios with these very light gluinos tend to feature lightest SUSY particles, or LSPs, with masses within a few hundred GeV of the gluino mass, which causes the decay products of the gluino to be of low energy (soft), and thus either are not reconstructed in the detector or do not satisfy the event selection of the searches. Additionally, these scenarios typically feature long, sometimes exotic, decay chains, in which the gluinos would decay to LSPs through one or several intermediate particles, typically squarks, neutralinos, or charginos; the production of a gluino in association with a squark also accounts for many nonexcluded scenarios with low-mass gluinos. Such scenarios would tend to predict more particles in the events, but with small transverse momenta than those in scenarios that are excluded; there is a trade-off between the average energy of particles and the multiplicity of particles.

Similar conclusions were made with regard to first- and second-generation squarks. It was found that our knowledge of the third generation squarks (top and bottom squarks) has not been significantly impacted by the CMS searches because  results from previous experiments, such as measurements of b quark properties, excluded most of the low-mass top and bottom squark scenarios before CMS analyses arrived on the scene. The lightest colored particle among the surviving pMSSM points has a mass of around 400 GeV. It was found that no mass range of the LSP can be excluded, including the range below 100 GeV. Scenarios with very light LSPs predict enormous cross sections of SUSY events, but the events have few or no properties to which our current analyses are sensitive. 

After a decomposition of the nonexcluded model space into principal processes, it was found that the second most common process in the pMSSM is a process for which there is no simplified model targeted by any of the CMS or ATLAS searches, namely, the direct production of one LSP and one chargino. In many cases, the chargino and LSP are nearly mass degenerate, which can lead to either soft, off-shell W bosons in the decay of the chargino, or in the case of extreme mass degeneracy, charginos with lifetimes as long as 10 mm. In either case, soft W bosons or long-lived charginos could provide the observables necessary to probe this difficult class of model points. Specifically, a set of event selection that includes initial-state radiation in conjunction with a reconstructed soft lepton or disappearing track traveling in a similar azimuthal direction as the missing transverse momentum, could define the starting point for such an analysis. 

Moreover, two general findings were made. The first is that searches for SUSY in the hadronic channel were found to be the most effective at probing the MSSM, by a significant margin. Second, many pMSSM points with very high cross sections survive the 7 and 8 TeV CMS searches because the they predict signal events that arise mostly in background-dominated kinematic regions, such as regions of low-$\Ht$, low-$\mht$, and low jet multiplicity. Because these kinematic regions are background-dominated, new discriminating variables may be required to allow for sensitivity to the nonexcluded signals predicted to appear in these regions. After making these findings, I spent much of my remaining PhD career developing and implementing methods that increase the sensitivity of hadronic searches in regions of low-$\Ht$, $\mht$, and $\njets$. 

I re-envisioned the rebalance and smear method, a procedure for data-driven QCD estimation, as a problem of Bayesian inference. I cast the problem of QCD estimation in terms of known and unknown quantities, constructing a prior for the probability distribution of the parton-level $\mht$ in QCD events, and a likelihood function built out of the full jet response functions. The result is a data-driven QCD event generator that more accurately models Standard Model multi-jet events than previous estimation methods, and reproduces the correct correlations between the magnitudes and directions of the jet four-vectors in the events. The method yields an accurate prediction even in the case that there is signal contamination in the data control samples from which the prediction is derived. This means that the method can be applied using real data events, thereby preserving the true correlations of jets and the jet multiplicity exhibited by nature, quantities that are notoriously poorly-modeled in Monte Carlo event simulation. Moreover, the method can be applied repeatedly such that the statistical uncertainty in the QCD prediction in any kinematic region can be obtained with negligible statistical uncertainty. This method has been developed in the context of the CMS multi-jet $+$ $\mht$ search for supersymmetry at $\sqrt{s}=13$ TeV \cite{Khachatryan:2016kdk}. 

I extended the method for the estimation of the $\zinv$ background in an analysis searching for evidence of top squarks in the CMS data~\cite{CMS:2016nhb}, so that the final prediction is constrained by real events containing two electrons. Previously, the estimation was constrained by samples of events with two muons, so this method provides both a check for and an improvement in the $\zinv$ background prediction. The main hurdles were accurately modeling the acceptance, reconstruction efficiency, and isolation efficiency of Z bosons decaying into pairs of electrons. The result is a parametrization of the efficiency that can be applied in data as a way to predict the Standard Model $\zinv$ distributions, including for observables that rely on complicated relations between properties of the jets, such as multivariate discriminants. Furthermore, the efficiency parameterization may be applied to the muon samples in future versions of the hadronic top squark search, since the level of consistency between the prediction and the expectation is high, including in regions of low-$\mht$ and $\Ht$.

I made use of, and built upon, methods of measuring the efficiency of triggers in a way that either avoids or properly accounts for bias that may be introduced choice of the control sample used to derived the efficiency. The result is an estimate of the trigger efficiency that is a smoothly varying function than can depend on multiple final state observables.  The benefits of this technique include an easing of the difficulty of conducting an analysis in kinematic regions in which the trigger is not fully efficient. Often these regions are avoided in analysis, sometimes out of a concern that unknown bias affect the traditional, binned efficiency measurements, and sometimes out of the conventional notion that such regions are not populated by a large number of signal events, and so aren't worth analyzing. However, SUSY may exist and manifest itself in these difficult regions, and so tools like these may be key if we decide to rise to the challenge.

I established that multivariate techniques can be employed as a means of significantly increasing the sensitivity of searches in regions of low $\mht$ and $\Ht$. Multivariate discriminants that take jet properties as input can provide good signal/background discrimination for the main backgrounds of these regions, which are dominated by Standard Model QCD and $\zinv$ events. Moreover, the data-driven methods of background estimation presented in this document and employed high-$\met$ regions, have been shown to accurately model the background distributions of such discriminants.

I contributed to detector research and development, helping to conduct test beam activities and analysis of data for the CMS Phase II upgrade. To this end, I authored an event display that allowed the data to be viewed during and after their collection. The event display has been adopted for use in other testbeam experiments, and I look forward to employing it in future work.

Finally, I worked on a project~\cite{Dumont:2014tja} to help to make the results of published CMS searches more accessible to theorists. This project established a public framework in which anyone can write code that calculates the signal efficiency$\times$acceptance of a CMS or ATLAS analysis.  Once an analysis has been re-implemented and validated, theorists, or anyone interested in how a given model is constrained by the CMS data, can quickly improve their understanding of their model.

Much work has been carried out to learn the true nature of electroweak symmetry breaking. Clearly, a great deal more work remains to be done before we will understand this curious aspect of nature. But there is good reason to believe that with ever more effective and rigorous analyses, with increasingly bold ideas and careful reasoning, we may be able to state conclusively, by the end of the LHC era, that either we have discovered supersymmetry or that the weak-scale SUSY hypothesis has been ruled out.

